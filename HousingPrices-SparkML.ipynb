{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Prices - Spark ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a continuation of the Housing Prices prediction. In this case, we will be looking at how to build similar models from the main scitkit learn example, but in this case using Spark ML.\n",
    "\n",
    "First, we extract the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://bea7d995f668:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1570730255655)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@385c9719\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_key: String = housing_data_raw.csv\n",
       "df: org.apache.spark.sql.DataFrame = [_c0: int, address: string ... 17 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data_key = \"housing_data_raw.csv\"\n",
    "\n",
    "val df = spark.read\n",
    ".format(\"csv\")\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".load(s\"./$data_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will drop the same outliers as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.sql.functions._\n",
       "drop_outliers: (data: org.apache.spark.sql.DataFrame)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n",
       "housing: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_c0: int, address: string ... 17 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "def drop_outliers(data: DataFrame) = {\n",
    "    val drop = List(1618, 3405,10652, 954, 11136, 5103, 916, 10967, 7383, 1465, 8967, 8300, 4997)\n",
    "    \n",
    "    data.filter(not($\"_c0\".isin(drop:_*)))\n",
    "}\n",
    "\n",
    "val housing = drop_outliers(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will cleanup the `lastsolddate` column so that it is a numeric value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "housing_dateint: org.apache.spark.sql.DataFrame = [_c0: int, address: string ... 18 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val housing_dateint = housing.withColumn(\"lastsolddateint\", unix_timestamp($\"lastsolddate\",\"MM/dd/yy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then remove data that may not be helpful (at least initially) for building models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drop_geog: (data: org.apache.spark.sql.DataFrame, keep: List[String])org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_geog(data: DataFrame, keep: List[String] = List()) = {\n",
    "    val removeList = List(\"info\",\"address\",\"z_address\",\"longitude\",\"latitude\",\"neighborhood\",\n",
    "                          \"lastsolddate\",\"zipcode\",\"zpid\",\"usecode\", \"zestimate\",\"zindexvalue\")\n",
    "    .filter(!keep.contains(_))\n",
    "    \n",
    "    data.drop(removeList: _*)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "housing_dropgeo: org.apache.spark.sql.DataFrame = [_c0: int, bathrooms: double ... 6 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val housing_dropgeo = drop_geog(housing_dateint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move onto the Spark ML aspects. In order to train a model, the data needs to be formatted in a Spark DataFrame (which is not that much different conceptually from a Pandas DataFrame). But the features also need to be in a single column in the format of a Vector, which is essentially a list of all the features for that row. We can assemble the Vector with the following code, putting all the columns except the label, `lastsoldprice`, into the Vector. At the same time, we will split our data into a training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "train_test_split: (data: org.apache.spark.sql.DataFrame)(org.apache.spark.sql.DataFrame, org.apache.spark.sql.DataFrame)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "def train_test_split(data: DataFrame) = {\n",
    "    \n",
    "    val assembler = new VectorAssembler().\n",
    "       setInputCols(data.drop(\"lastsoldprice\").columns).\n",
    "       setOutputCol(\"features\")\n",
    "    \n",
    "    val Array(train, test) = data.randomSplit(Array(0.8, 0.2), seed = 30)\n",
    "\n",
    "    (assembler.transform(train), assembler.transform(test))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train: org.apache.spark.sql.DataFrame = [_c0: int, bathrooms: double ... 7 more fields]\n",
       "test: org.apache.spark.sql.DataFrame = [_c0: int, bathrooms: double ... 7 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val (train, test) = train_test_split(housing_dropgeo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will get to some actual machine learning. First, linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 857356.2890199891\n",
      "R^2 on test data = 0.31933500943383086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.LinearRegression\n",
       "lr: org.apache.spark.ml.regression.LinearRegression = linReg_9aed1af51530\n",
       "lrModel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_9aed1af51530\n",
       "predictions: org.apache.spark.sql.DataFrame = [_c0: int, bathrooms: double ... 8 more fields]\n",
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "rmse: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_441bb31d3542\n",
       "r2: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_761a46688c27\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "\n",
    "val lr = new LinearRegression()\n",
    "    .setLabelCol(\"lastsoldprice\")\n",
    "    .setFeaturesCol(\"features\")\n",
    "\n",
    "val lrModel = lr.fit(train)\n",
    "val predictions = lrModel.transform(test)\n",
    "\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "val rmse = new RegressionEvaluator()\n",
    "  .setLabelCol(\"lastsoldprice\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"rmse\")\n",
    "\n",
    "val r2 = new RegressionEvaluator()\n",
    "  .setLabelCol(\"lastsoldprice\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"r2\")\n",
    "\n",
    "println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse.evaluate(predictions))\n",
    "println(\"R^2 on test data = \" + r2.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, we will do the same again but with a reusable method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 857348.5768801493\n",
      "R^2 on test data = 0.31934725489128823\n",
      "{\n",
      "\tlinReg_359b4e462342-aggregationDepth: 2,\n",
      "\tlinReg_359b4e462342-elasticNetParam: 1.0,\n",
      "\tlinReg_359b4e462342-epsilon: 1.35,\n",
      "\tlinReg_359b4e462342-featuresCol: features,\n",
      "\tlinReg_359b4e462342-fitIntercept: true,\n",
      "\tlinReg_359b4e462342-labelCol: lastsoldprice,\n",
      "\tlinReg_359b4e462342-loss: squaredError,\n",
      "\tlinReg_359b4e462342-maxIter: 10000,\n",
      "\tlinReg_359b4e462342-predictionCol: prediction,\n",
      "\tlinReg_359b4e462342-regParam: 10.0,\n",
      "\tlinReg_359b4e462342-solver: auto,\n",
      "\tlinReg_359b4e462342-standardization: true,\n",
      "\tlinReg_359b4e462342-tol: 1.0E-6\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Predictor\n",
       "import org.apache.spark.ml.PredictionModel\n",
       "import org.apache.spark.ml.linalg.Vector\n",
       "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
       "import org.apache.spark.ml.param.ParamMap\n",
       "import org.apache.spark.ml.regression.LinearRegression\n",
       "train_eval: [R <: org.apache.spark.ml.Predictor[org.apache.spark.ml.linalg.Vector,R,M], M <: org.apache.spark.ml.PredictionModel[org.apache.spark.ml.linalg.Vector,M]](predictor: org.apache.spark.ml.Predictor[org.apache.spark.ml.linalg.Vector,R,M], paramMap: Array[org.apache.spark.ml.param.ParamMap], train: org.apache.spark.sql.DataFrame, test: org.apache.spark.sql.DataFrame)org.apache.spark.ml.Model[_]\n",
       "lr: org.apache.spark.ml.regression.LinearRegression = linReg_359b4e462342\n",
       "lrParamMap: Array[org.apache.s..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Predictor\n",
    "import org.apache.spark.ml.PredictionModel\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "\n",
    "def train_eval[R <: Predictor[Vector, R, M],\n",
    "               M <: PredictionModel[Vector, M]](\n",
    "    predictor: Predictor[Vector, R, M],\n",
    "    paramMap: Array[ParamMap],\n",
    "    train: DataFrame, \n",
    "    test: DataFrame) = {\n",
    "\n",
    "    val cv = new CrossValidator()\n",
    "      .setEstimator( predictor    \n",
    "                    .setLabelCol(\"lastsoldprice\")\n",
    "                    .setFeaturesCol(\"features\"))\n",
    "      .setEvaluator(new RegressionEvaluator()\n",
    "          .setLabelCol(\"lastsoldprice\")\n",
    "          .setPredictionCol(\"prediction\")\n",
    "          .setMetricName(\"rmse\"))\n",
    "      .setEstimatorParamMaps(paramMap)\n",
    "      .setNumFolds(5)\n",
    "      .setParallelism(2)\n",
    "\n",
    "    val cvModel = cv.fit(train)\n",
    "    val predictions = cvModel.transform(test)\n",
    "    \n",
    "    println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse.evaluate(predictions))\n",
    "    println(\"R^2 on test data = \" + r2.evaluate(predictions))\n",
    "\n",
    "    val bestModel = cvModel.bestModel\n",
    "    \n",
    "    println(bestModel.extractParamMap)\n",
    "    \n",
    "    bestModel\n",
    "}\n",
    "\n",
    "val lr = new LinearRegression()\n",
    "\n",
    "val lrParamMap = new ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, Array(10, 1, 0.1, 0.01, 0.001))\n",
    "    .addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0))\n",
    "    .addGrid(lr.maxIter, Array(10000, 250000))\n",
    "    .build()\n",
    "\n",
    "train_eval(lr, lrParamMap, train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we can try again with different machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 759685.8395738212\n",
      "R^2 on test data = 0.46558480196241925\n",
      "{\n",
      "\tdtr_c14d3cc21045-cacheNodeIds: false,\n",
      "\tdtr_c14d3cc21045-checkpointInterval: 10,\n",
      "\tdtr_c14d3cc21045-featuresCol: features,\n",
      "\tdtr_c14d3cc21045-impurity: variance,\n",
      "\tdtr_c14d3cc21045-labelCol: lastsoldprice,\n",
      "\tdtr_c14d3cc21045-maxBins: 32,\n",
      "\tdtr_c14d3cc21045-maxDepth: 5,\n",
      "\tdtr_c14d3cc21045-maxMemoryInMB: 256,\n",
      "\tdtr_c14d3cc21045-minInfoGain: 0.0,\n",
      "\tdtr_c14d3cc21045-minInstancesPerNode: 1,\n",
      "\tdtr_c14d3cc21045-predictionCol: prediction,\n",
      "\tdtr_c14d3cc21045-seed: 926680331\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.DecisionTreeRegressor\n",
       "decisionTree: org.apache.spark.ml.regression.DecisionTreeRegressor = dtr_c14d3cc21045\n",
       "dtParamMap: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\n",
       "})\n",
       "res3: org.apache.spark.ml.Model[_] = DecisionTreeRegressionModel (uid=dtr_c14d3cc21045) of depth 5 with 63 nodes\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.DecisionTreeRegressor\n",
    "\n",
    "val decisionTree = new DecisionTreeRegressor()\n",
    "val dtParamMap = new ParamGridBuilder().build()\n",
    "train_eval(decisionTree, dtParamMap, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 647133.830611256\n",
      "R^2 on test data = 0.6122079099308858\n",
      "{\n",
      "\trfr_162d5da97e7e-cacheNodeIds: false,\n",
      "\trfr_162d5da97e7e-checkpointInterval: 10,\n",
      "\trfr_162d5da97e7e-featureSubsetStrategy: auto,\n",
      "\trfr_162d5da97e7e-featuresCol: features,\n",
      "\trfr_162d5da97e7e-impurity: variance,\n",
      "\trfr_162d5da97e7e-labelCol: lastsoldprice,\n",
      "\trfr_162d5da97e7e-maxBins: 64,\n",
      "\trfr_162d5da97e7e-maxDepth: 10,\n",
      "\trfr_162d5da97e7e-maxMemoryInMB: 256,\n",
      "\trfr_162d5da97e7e-minInfoGain: 0.0,\n",
      "\trfr_162d5da97e7e-minInstancesPerNode: 1,\n",
      "\trfr_162d5da97e7e-numTrees: 100,\n",
      "\trfr_162d5da97e7e-predictionCol: prediction,\n",
      "\trfr_162d5da97e7e-seed: 235498149,\n",
      "\trfr_162d5da97e7e-subsamplingRate: 1.0\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.RandomForestRegressor\n",
       "randomForest: org.apache.spark.ml.regression.RandomForestRegressor = rfr_162d5da97e7e\n",
       "rfParamMap: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\trfr_162d5da97e7e-maxBins: 4,\n",
       "\trfr_162d5da97e7e-maxDepth: 2,\n",
       "\trfr_162d5da97e7e-numTrees: 1\n",
       "}, {\n",
       "\trfr_162d5da97e7e-maxBins: 16,\n",
       "\trfr_162d5da97e7e-maxDepth: 2,\n",
       "\trfr_162d5da97e7e-numTrees: 1\n",
       "}, {\n",
       "\trfr_162d5da97e7e-maxBins: 32,\n",
       "\trfr_162d5da97e7e-maxDepth: 2,\n",
       "\trfr_162d5da97e7e-numTrees: 1\n",
       "}, {\n",
       "\trfr_162d5da97e7e-maxBins: 64,\n",
       "\trfr_162d5da97e7e-maxDepth: 2,\n",
       "\trfr_162d5da97e7e-numTrees: 1\n",
       "}, {\n",
       "\trfr_162d5da97e7e-maxBins: 4,\n",
       "\trfr_162d5da97e7e-maxDepth: 2,\n",
       "\trfr_162d5da97e7e-numTrees: 10\n",
       "}, {\n",
       "\trfr_162d5da97e7e-maxBins: 16,\n",
       "\trfr_162d5da97e7e-maxDepth: 2,\n",
       "\trfr_162d5da97e7e-numTrees: 10\n",
       "}, {\n",
       "\trfr_..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.RandomForestRegressor\n",
    "\n",
    "val randomForest = new RandomForestRegressor()\n",
    "\n",
    "val rfParamMap = new ParamGridBuilder()\n",
    "    .addGrid(randomForest.maxBins, Array(4, 16, 32, 64))\n",
    "    .addGrid(randomForest.numTrees, Array(1, 10, 100))\n",
    "    .addGrid(randomForest.maxDepth, Array(2, 5, 10))\n",
    "    .build()\n",
    "\n",
    "train_eval(randomForest, rfParamMap, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 703037.6456894034\n",
      "R^2 on test data = 0.5423137139558296\n",
      "{\n",
      "\tgbtr_4a77c4a76717-cacheNodeIds: false,\n",
      "\tgbtr_4a77c4a76717-checkpointInterval: 10,\n",
      "\tgbtr_4a77c4a76717-featureSubsetStrategy: all,\n",
      "\tgbtr_4a77c4a76717-featuresCol: features,\n",
      "\tgbtr_4a77c4a76717-impurity: variance,\n",
      "\tgbtr_4a77c4a76717-labelCol: lastsoldprice,\n",
      "\tgbtr_4a77c4a76717-lossType: squared,\n",
      "\tgbtr_4a77c4a76717-maxBins: 32,\n",
      "\tgbtr_4a77c4a76717-maxDepth: 5,\n",
      "\tgbtr_4a77c4a76717-maxIter: 20,\n",
      "\tgbtr_4a77c4a76717-maxMemoryInMB: 256,\n",
      "\tgbtr_4a77c4a76717-minInfoGain: 0.0,\n",
      "\tgbtr_4a77c4a76717-minInstancesPerNode: 1,\n",
      "\tgbtr_4a77c4a76717-predictionCol: prediction,\n",
      "\tgbtr_4a77c4a76717-seed: -131597770,\n",
      "\tgbtr_4a77c4a76717-stepSize: 0.1,\n",
      "\tgbtr_4a77c4a76717-subsamplingRate: 1.0,\n",
      "\tgbtr_4a77c4a76717-validationTol: 0.01\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.GBTRegressor\n",
       "gradientBoost: org.apache.spark.ml.regression.GBTRegressor = gbtr_4a77c4a76717\n",
       "gbParamMap: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\trfr_162d5da97e7e-maxBins: 16,\n",
       "\trfr_162d5da97e7e-maxDepth: 5,\n",
       "\trfr_162d5da97e7e-minInfoGain: 0.0,\n",
       "\trfr_162d5da97e7e-numTrees: 5\n",
       "}, {\n",
       "\trfr_162d5da97e7e-maxBins: 32,\n",
       "\trfr_162d5da97e7e-maxDepth: 5,\n",
       "\trfr_162d5da97e7e-minInfoGain: 0.0,\n",
       "\trfr_162d5da97e7e-numTrees: 5\n",
       "}, {\n",
       "\trfr_162d5da97e7e-maxBins: 16,\n",
       "\trfr_162d5da97e7e-maxDepth: 5,\n",
       "\trfr_162d5da97e7e-minInfoGain: 0.1,\n",
       "\trfr_162d5da97e7e-numTrees: 5\n",
       "}, {\n",
       "\trfr_162d5da97e7e-maxBins: 32,\n",
       "\trfr_162d5da97e7e-maxDepth: 5,\n",
       "\trfr_162d5da97e7e-minInfoGain: 0.1,\n",
       "\trfr_162d5da97e7e-numTrees: 5\n",
       "}, {\n",
       "\trfr_162d5da97e7e-maxBins: 16,\n",
       "\trfr_162d5da97e7e-maxDepth: 5,\n",
       "\trfr_162d..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.GBTRegressor\n",
    "\n",
    "val gradientBoost = new GBTRegressor()\n",
    "\n",
    "val gbParamMap = new ParamGridBuilder()\n",
    "    .addGrid(randomForest.maxBins, Array(16, 32))\n",
    "    .addGrid(randomForest.numTrees, Array(5, 10, 100))\n",
    "    .addGrid(randomForest.maxDepth, Array(5, 10))\n",
    "    .addGrid(randomForest.minInfoGain, Array(0.0, 0.1, 0.5))\n",
    "    .build()\n",
    "\n",
    "train_eval(gradientBoost, gbParamMap, train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want better results, we will have to bring in the neighborhood data as we did in the scitket learn example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "housing_neighborhood: org.apache.spark.sql.DataFrame = [_c0: int, bathrooms: double ... 7 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val housing_neighborhood = drop_geog(housing_dateint, List(\"neighborhood\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------------+-------------+----------+---------+---------------+------------------+\n",
      "|_c0|bathrooms|bedrooms|finishedsqft|lastsoldprice|totalrooms|yearbuilt|lastsolddateint|neighborhoodVector|\n",
      "+---+---------+--------+------------+-------------+----------+---------+---------------+------------------+\n",
      "|  2|      2.0|     2.0|      1043.0|    1300000.0|       4.0|   2007.0|     1455667200|    (70,[1],[1.0])|\n",
      "|  5|      1.0|     1.0|       903.0|     750000.0|       3.0|   2004.0|     1455667200|    (70,[1],[1.0])|\n",
      "|  7|      4.0|     3.0|      1425.0|    1495000.0|       6.0|   2003.0|     1455667200|    (70,[8],[1.0])|\n",
      "|  9|      3.0|     3.0|      2231.0|    2700000.0|      10.0|   1927.0|     1455667200|    (70,[8],[1.0])|\n",
      "| 11|      3.0|     3.0|      1300.0|    1530000.0|       4.0|   1900.0|     1455667200|    (70,[2],[1.0])|\n",
      "| 12|      1.0|     2.0|      1250.0|     460000.0|       5.0|   1924.0|     1455667200|   (70,[34],[1.0])|\n",
      "| 13|      1.0|     3.0|      1032.0|     532000.0|       6.0|   1939.0|     1455667200|   (70,[46],[1.0])|\n",
      "| 14|      1.0|     2.0|      1200.0|    1050000.0|       5.0|   1924.0|     1455667200|   (70,[36],[1.0])|\n",
      "| 15|      3.5|     4.0|      2700.0|    3500000.0|       9.0|   1912.0|     1455667200|    (70,[5],[1.0])|\n",
      "| 16|      2.0|     3.0|      2016.0|    1500000.0|       7.0|   1890.0|     1455667200|    (70,[9],[1.0])|\n",
      "| 18|      1.0|     3.0|      1798.0|     848000.0|       8.0|   1953.0|     1455667200|   (70,[21],[1.0])|\n",
      "| 19|      1.0|     1.0|       761.0|    1000000.0|       4.0|   2008.0|     1455667200|    (70,[1],[1.0])|\n",
      "| 24|      1.0|     1.0|       780.0|     863000.0|       4.0|   1981.0|     1439337600|    (70,[7],[1.0])|\n",
      "| 25|      5.0|     5.0|      5786.0|    4888000.0|      12.0|   1926.0|     1455580800|   (70,[31],[1.0])|\n",
      "| 26|      2.0|     2.0|      1688.0|    1000000.0|       6.0|   1927.0|     1455580800|   (70,[28],[1.0])|\n",
      "| 27|      3.0|     4.0|      1619.0|     210000.0|       7.0|   1966.0|     1455580800|   (70,[37],[1.0])|\n",
      "| 33|      1.0|     0.0|       398.0|     525000.0|       4.0|   2008.0|     1455235200|   (70,[11],[1.0])|\n",
      "| 34|      4.5|     4.0|      2615.0|    2300000.0|       9.0|   1906.0|     1455235200|    (70,[0],[1.0])|\n",
      "| 35|      2.0|     2.0|      1252.0|    1450000.0|       4.0|   2002.0|     1455235200|   (70,[24],[1.0])|\n",
      "| 36|      2.0|     3.0|      1444.0|    2500000.0|       6.0|   2009.0|     1455235200|   (70,[24],[1.0])|\n",
      "+---+---------+--------+------------+-------------+----------+---------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
       "import org.apache.spark.ml.feature.StringIndexer\n",
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_2f7196545152\n",
       "encoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_592c053827bf\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_920d33d8cbee\n",
       "housingEncoded: org.apache.spark.sql.DataFrame = [_c0: int, bathrooms: double ... 7 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "\n",
    "val indexer = new StringIndexer().setInputCol(\"neighborhood\").setOutputCol(\"neighborhoodIndex\")\n",
    "\n",
    "val encoder = new OneHotEncoderEstimator()\n",
    "  .setInputCols(Array(indexer.getOutputCol))\n",
    "  .setOutputCols(Array(\"neighborhoodVector\"))\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(indexer, encoder))\n",
    "\n",
    "val housingEncoded = pipeline.fit(housing_neighborhood).transform(housing_neighborhood)\n",
    ".drop(\"neighborhoodIndex\")\n",
    ".drop(\"neighborhood\")\n",
    "\n",
    "housingEncoded.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a new train and test set, and try again with the same algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_neighborhood: org.apache.spark.sql.DataFrame = [_c0: int, bathrooms: double ... 8 more fields]\n",
       "test_neighborhood: org.apache.spark.sql.DataFrame = [_c0: int, bathrooms: double ... 8 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val (train_neighborhood, test_neighborhood) = train_test_split(housingEncoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 754869.9632285038\n",
      "R^2 on test data = 0.4723389619596349\n",
      "{\n",
      "\tlinReg_359b4e462342-aggregationDepth: 2,\n",
      "\tlinReg_359b4e462342-elasticNetParam: 0.0,\n",
      "\tlinReg_359b4e462342-epsilon: 1.35,\n",
      "\tlinReg_359b4e462342-featuresCol: features,\n",
      "\tlinReg_359b4e462342-fitIntercept: true,\n",
      "\tlinReg_359b4e462342-labelCol: lastsoldprice,\n",
      "\tlinReg_359b4e462342-loss: squaredError,\n",
      "\tlinReg_359b4e462342-maxIter: 10000,\n",
      "\tlinReg_359b4e462342-predictionCol: prediction,\n",
      "\tlinReg_359b4e462342-regParam: 10.0,\n",
      "\tlinReg_359b4e462342-solver: auto,\n",
      "\tlinReg_359b4e462342-standardization: true,\n",
      "\tlinReg_359b4e462342-tol: 1.0E-6\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res7: org.apache.spark.ml.Model[_] = linReg_359b4e462342\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_eval(lr, lrParamMap, train_neighborhood, test_neighborhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 722171.2606321493\n",
      "R^2 on test data = 0.5170622654844328\n",
      "{\n",
      "\tdtr_c14d3cc21045-cacheNodeIds: false,\n",
      "\tdtr_c14d3cc21045-checkpointInterval: 10,\n",
      "\tdtr_c14d3cc21045-featuresCol: features,\n",
      "\tdtr_c14d3cc21045-impurity: variance,\n",
      "\tdtr_c14d3cc21045-labelCol: lastsoldprice,\n",
      "\tdtr_c14d3cc21045-maxBins: 32,\n",
      "\tdtr_c14d3cc21045-maxDepth: 5,\n",
      "\tdtr_c14d3cc21045-maxMemoryInMB: 256,\n",
      "\tdtr_c14d3cc21045-minInfoGain: 0.0,\n",
      "\tdtr_c14d3cc21045-minInstancesPerNode: 1,\n",
      "\tdtr_c14d3cc21045-predictionCol: prediction,\n",
      "\tdtr_c14d3cc21045-seed: 926680331\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res8: org.apache.spark.ml.Model[_] = DecisionTreeRegressionModel (uid=dtr_c14d3cc21045) of depth 5 with 63 nodes\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_eval(decisionTree, dtParamMap, train_neighborhood, test_neighborhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 581188.983582857\n",
      "R^2 on test data = 0.6872153115815951\n",
      "{\n",
      "\trfr_162d5da97e7e-cacheNodeIds: false,\n",
      "\trfr_162d5da97e7e-checkpointInterval: 10,\n",
      "\trfr_162d5da97e7e-featureSubsetStrategy: auto,\n",
      "\trfr_162d5da97e7e-featuresCol: features,\n",
      "\trfr_162d5da97e7e-impurity: variance,\n",
      "\trfr_162d5da97e7e-labelCol: lastsoldprice,\n",
      "\trfr_162d5da97e7e-maxBins: 64,\n",
      "\trfr_162d5da97e7e-maxDepth: 10,\n",
      "\trfr_162d5da97e7e-maxMemoryInMB: 256,\n",
      "\trfr_162d5da97e7e-minInfoGain: 0.0,\n",
      "\trfr_162d5da97e7e-minInstancesPerNode: 1,\n",
      "\trfr_162d5da97e7e-numTrees: 100,\n",
      "\trfr_162d5da97e7e-predictionCol: prediction,\n",
      "\trfr_162d5da97e7e-seed: 235498149,\n",
      "\trfr_162d5da97e7e-subsamplingRate: 1.0\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res9: org.apache.spark.ml.Model[_] = RandomForestRegressionModel (uid=rfr_162d5da97e7e) with 100 trees\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_eval(randomForest, rfParamMap, train_neighborhood, test_neighborhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 636055.9695573623\n",
      "R^2 on test data = 0.6253709908240936\n",
      "{\n",
      "\tgbtr_4a77c4a76717-cacheNodeIds: false,\n",
      "\tgbtr_4a77c4a76717-checkpointInterval: 10,\n",
      "\tgbtr_4a77c4a76717-featureSubsetStrategy: all,\n",
      "\tgbtr_4a77c4a76717-featuresCol: features,\n",
      "\tgbtr_4a77c4a76717-impurity: variance,\n",
      "\tgbtr_4a77c4a76717-labelCol: lastsoldprice,\n",
      "\tgbtr_4a77c4a76717-lossType: squared,\n",
      "\tgbtr_4a77c4a76717-maxBins: 32,\n",
      "\tgbtr_4a77c4a76717-maxDepth: 5,\n",
      "\tgbtr_4a77c4a76717-maxIter: 20,\n",
      "\tgbtr_4a77c4a76717-maxMemoryInMB: 256,\n",
      "\tgbtr_4a77c4a76717-minInfoGain: 0.0,\n",
      "\tgbtr_4a77c4a76717-minInstancesPerNode: 1,\n",
      "\tgbtr_4a77c4a76717-predictionCol: prediction,\n",
      "\tgbtr_4a77c4a76717-seed: -131597770,\n",
      "\tgbtr_4a77c4a76717-stepSize: 0.1,\n",
      "\tgbtr_4a77c4a76717-subsamplingRate: 1.0,\n",
      "\tgbtr_4a77c4a76717-validationTol: 0.01\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res10: org.apache.spark.ml.Model[_] = GBTRegressionModel (uid=gbtr_4a77c4a76717) with 20 trees\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_eval(gradientBoost, gbParamMap, train_neighborhood, test_neighborhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
